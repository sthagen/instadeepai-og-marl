{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Offline Multi-Agent Reinforcement Learning Datasets and Baselines</p> <p> </p> <p> </p>"},{"location":"#going-off-the-grid","title":"Going Off-the-Grid! \ud83e\udd16 \u26a1 \ud83d\udd0c \ud83d\udd0b","text":"<p>Offline MARL holds great promise for real-world applications by utilising static datasets to build decentralised controllers of complex multi-agent systems. However, currently offline MARL lacks a standardised benchmark for measuring meaningful research progress. Off-the-Grid MARL (OG-MARL) fills this gap by providing a diverse suite of datasets with baselines on popular MARL benchmark environments in one place, with a unified API and an easy-to-use set of tools.</p> <p>OG-MARL forms part of the InstaDeep MARL ecosystem, developed jointly with the open-source community. To join us in these efforts, reach out, raise issues or just \ud83c\udf1f to stay up to date with the latest developments!</p>"},{"location":"#quickstart","title":"Quickstart \ud83c\udfce\ufe0f","text":"<p>Clone this repository.</p> <p><code>git clone https://github.com/instadeepai/og-marl.git</code></p> <p>Install <code>og-marl</code> and its dependencies. We tested <code>og-marl</code> with Python 3.9. Consider using a <code>conda</code> virtual environment.</p> <p><code>pip install -e .</code></p> <p><code>pip install flashbax==0.1.2</code></p> <p>Download environment dependencies. We will use SMACv1 in this example.</p> <p><code>bash install_environments/smacv1.sh</code></p> <p>Download a dataset.</p> <p><code>python examples/download_dataset.py --env=smac_v1 --scenario=3m</code></p> <p>Run a baseline. In this example we will run MAICQ.</p> <p><code>python baselines/main.py --env=smac_v1 --scenario=3m --dataset=Good --system=maicq</code></p>"},{"location":"#dataset-api","title":"Dataset API","text":"<p>We provide a simple demonstrative notebook of how to use OG-MARL's dataset API here:</p> <p></p>"},{"location":"#datasets","title":"Datasets \ud83c\udfa5","text":"<p>We have generated datasets on a diverse set of popular MARL environments. A list of currently supported environments is included in the table below. It is well known from the single-agent offline RL literature that the quality of experience in offline datasets can play a large role in the final performance of offline RL algorithms. Therefore in OG-MARL, for each environment and scenario, we include a range of dataset distributions including <code>Good</code>, <code>Medium</code>, <code>Poor</code> and <code>Replay</code> datasets in order to benchmark offline MARL algorithms on a range of different dataset qualities. For more information on why we chose to include each environment and its task properties, please read our accompanying paper.</p> <p></p>"},{"location":"#dataset-backends","title":"Dataset Backends \ud83d\udd0c","text":"<p>We are in the process of migrating our datasets from TF Records to Flashbax Vaults. Flashbax Vaults have the advantage of being significantly more flexible than the TF Record Datasets.</p>"},{"location":"#flashbax-vaults","title":"Flashbax Vaults \u26a1","text":"Environment Scenario Agents Act Obs Reward Types Repo \ud83d\udd2bSMAC v1 3m  8m  2s3z  5m_vs_6m  27m_vs_30m  3s5z_vs_3s6z  2c_vs_64zg 3  8  5  5  27  8  2 Discrete Vector Dense Homog  Homog  Heterog  Homog  Homog  Heterog  Homog source \ud83d\udca3SMAC v2 terran_5_vs_5  zerg_5_vs_5  terran_10_vs_10 5  5  10 Discrete Vector Dense Heterog source \ud83d\ude85Flatland 3 Trains   5 Trains 3  5 Discrete Vector Sparse Homog source \ud83d\udc1cMAMuJoCo 2-HalfCheetah  2-Ant  4-Ant 2  2  4 Cont. Vector Dense Heterog  Homog  Homog source \ud83d\udc3bPettingZoo Pursuit   Co-op Pong 8  2 Discrete  Discrete Pixels  Pixels Dense Homog  Heterog source"},{"location":"#legacy-datasets-still-to-be-migrated-to-vault","title":"Legacy Datasets (still to be migrated to Vault) \ud83d\udc74","text":"Environment Scenario Agents Act Obs Reward Types Repo \ud83d\udc3bPettingZoo PistonBall  KAZ 15  2 Cont.  Discrete Pixels  Vector Dense Homog  Heterog source \ud83c\udfd9\ufe0fCityLearn 2022_all_phases 17 Cont. Vector Dense Homog source \ud83d\udd0cVoltage Control case33_3min_final 6 Cont. Vector Dense Homog source \ud83d\udd34MPE simple_adversary 3 Discrete. Vector Dense Competitive source"},{"location":"#dataset-and-vault-locations","title":"Dataset and Vault Locations","text":"<p>For OG-MARL's systems, we require the following dataset file structure:</p> <pre><code>examples/\n    |_&gt; ...\nog_marl/\n    |_&gt; ...\nvaults/\n    |_&gt; smac_v1/\n        |_&gt; 3m.vlt/\n        |   |_&gt; Good/\n        |   |_&gt; Medium/\n        |   |_&gt; Poor/\n        |_&gt; ...\n    |_&gt; smac_v2/\n        |_&gt; terran_5_vs_5.vlt/\n        |   |_&gt; Good/\n        |   |_&gt; Medium/\n        |   |_&gt; Poor/\n        |_&gt; ...\n...\n</code></pre>"},{"location":"#see-also","title":"See Also \ud83d\udd0e","text":"<p>InstaDeep's MARL ecosystem in JAX. In particular, we suggest users check out the following sister repositories:</p> <ul> <li>\ud83e\udd81 Mava: a research-friendly codebase for distributed MARL in JAX.</li> <li>\ud83c\udf34 Jumanji: a diverse suite of scalable reinforcement learning environments in JAX.</li> <li>\ud83d\ude0e Matrax: a collection of matrix games in JAX.</li> <li>\ud83d\udd26 Flashbax: accelerated replay buffers in JAX.</li> <li>\ud83d\udcc8 MARL-eval: standardised experiment data aggregation and visualisation for MARL.</li> </ul> <p>Related. Other libraries related to accelerated MARL in JAX.</p> <ul> <li>\ud83e\udd8a JaxMARL: accelerated MARL environments with baselines in JAX.</li> <li>\u265f\ufe0f  Pgx: JAX implementations of classic board games, such as Chess, Go and Shogi.</li> <li>\ud83d\udd3c Minimax: JAX implementations of autocurricula baselines for RL.</li> </ul>"},{"location":"#citing-og-marl","title":"Citing OG-MARL","text":"<p>If you use OG-MARL in your work, please cite the library using:</p> <pre><code>@inproceedings{formanek2023ogmarl,\n    author = {Formanek, Claude and Jeewa, Asad and Shock, Jonathan and Pretorius, Arnu},\n    title = {Off-the-Grid MARL: Datasets and Baselines for Offline Multi-Agent Reinforcement Learning},\n    year = {2023},\n    publisher = {AAMAS},\n    booktitle = {Extended Abstract at the 2023 International Conference on Autonomous Agents and Multiagent Systems},\n}\n</code></pre> <p></p>"},{"location":"#acknowledgements","title":"Acknowledgements \ud83d\ude4f","text":"<p>The development of this library was supported with Cloud TPUs from Google's TPU Research Cloud (TRC) \ud83c\udf24.</p>"},{"location":"api/","title":"API Reference (Coming Soon)","text":""},{"location":"datasets/","title":"Datasets","text":"Website Page Datasets SMAC V1 <ul> <li>3m</li> <li>8m</li> <li>5m_vs_6m</li> <li>2s3z</li> <li>3s5z_vs_3s6z</li> <li>2c_vs_64zg</li> <li>27m_vs_30m</li> </ul> SMAC V2 <ul> <li>terran_5_vs_5</li> <li>zerg_5_vs_5</li> <li>terran_10_vs_10</li> </ul> Flatland <ul> <li>3_trains</li> <li>5_trains</li> </ul> PettingZoo <ul> <li>pursuit</li> <li>pistonball</li> <li>coop_pong</li> <li>kaz</li> </ul> MaMuJoCo <ul> <li>2_halfcheetah</li> <li>2_ant</li> <li>4_ant</li> </ul> Voltage Control <ul> <li>case33_3min_final</li> </ul> City Learn <ul> <li>2022_all_phases</li> </ul> MPE <ul> <li>simple_adversary</li> </ul>"},{"location":"updates/","title":"Updates","text":""},{"location":"updates/#updates-06122023","title":"Updates [06/12/2023] \ud83d\udcf0","text":"<p>OG-MARL is a research tool that is under active development and therefore evolving quickly. We have several very exciting new features on the roadmap but sometimes when we introduce a new feature we may abruptly change how things work in OG-MARL. But in the interest of moving quickly, we believe this is an acceptable trade-off and ask our users to kindly be aware of this.</p> <p>The following is a list of the latest updates to OG-MARL:</p> <p>\u2705 We have removed several cumbersome dependencies from OG-MARL, including <code>reverb</code> and <code>launchpad</code>. This means that its significantly easier to install and use OG-MARL.</p> <p>\u2705 We added functionality to pre-load the TF Record datasets into a Cpprb replay buffer. This speeds up the time to sample the replay buffer by several orders of magnitude.</p> <p>\u2705 We have implemented our first set of JAX-based systems in OG-MARL. Our JAX systems use Flashbax as the replay buffer backend. Flashbax buffers are completely jit-able, which means that our JAX systems have fully integrated and jitted training and data sampling.</p> <p>\u2705 We have integrated MARL-eval into OG-MARL to standardise and simplify the reporting of experimental results.</p>"},{"location":"updates/#need-for-speed","title":"Need for Speed \ud83c\udfce\ufe0f","text":"<p>We have made our TF2 systems compatible with jit compilation. This combined with our new <code>cpprb</code> replay buffers have made our systems significantly faster. Furthermore, our JAX systems with tightly integrated replay sampling and training using Flashbax are even faster.</p> <p>Speed Comparison: for each setup, we trained MAICQ on the 8m Good dataset for 10k training steps and evaluated every 1k training steps for 4 episodes using a batch size of 256.</p> <p>Performance Comparison: In order to make sure performance between the TF2 system and the JAX system is the same, we trained both variants on each of the three datasets for 8m (Good, Medium and Poor). We then normalised the scores and aggregated the results using MARL-eval. The sample efficiency curves and the performance profiles are given below.</p>"},{"location":"baselines/flatland/","title":"Flatland Baseline Results","text":""},{"location":"baselines/mamujoco/","title":"MAMuJoCo Baseline Results","text":""},{"location":"baselines/pettingzoo/","title":"PettingZoo Baseline Results","text":""},{"location":"baselines/smac_v1/","title":"SMAC v1 Baseline Results","text":""},{"location":"baselines/smac_v2/","title":"SMAC v2 Baseline Results","text":""}]}